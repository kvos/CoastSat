{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CoastSat classifier\n",
    "\n",
    "In this notebook the CoastSat classifier is trained using satellite images from new sites. This can improve the accuracy of the shoreline detection if the users are experiencing issues with the default classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Sites for training:\n",
      "['COLLAROY.kml']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# coastsat modules\n",
    "from coastsat import SDS_download, SDS_preprocess, SDS_shoreline, SDS_tools, SDS_classify\n",
    "\n",
    "# filepaths \n",
    "filepath_images = os.path.join(os.getcwd(), 'data')\n",
    "filepath_train = os.path.join(os.getcwd(), 'examples', 'training_data')\n",
    "\n",
    "# settings\n",
    "settings ={'cloud_thresh':0.1, # percentage of cloudy pixels accepted on the image\n",
    "           'cloud_mask_issue':False, # set to True if problems with the default cloud mask \n",
    "           'inputs':{'filepath':filepath_images}, # folder where the images are stored\n",
    "           'labels':{'sand':1,'white-water':2,'water':3,'other land features':4}, # labels for the classifier\n",
    "           'flood_fill': True, # set to True to use the flood fill functionality\n",
    "           'tolerance':0.02, # if flood_fill set to True, this is the pixel intensity tolerance \n",
    "           'filepath_train':filepath_train} # folder where the labelled images are stored\n",
    "\n",
    "# read kml files for the training sites\n",
    "filepath_sites = os.path.join(os.getcwd(), 'examples', 'training_sites')\n",
    "train_sites = os.listdir(filepath_sites)\n",
    "print('Sites for training:\\n%s\\n'%train_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download images\n",
    "\n",
    "For each site on which you want to train the classifier, save a .kml file with the region of interest (5 vertices clockwise, first and last points are the same, can be created from Google myMaps) in the folder *\\training_sites*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sites for training:\n",
      "['COLLAROY.kml']\n",
      "COLLAROY.kml\n",
      "Downloading images:\n",
      "L8: 6 images\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "# dowload images at the sites\n",
    "filepath = os.path.join(os.getcwd(), 'data')\n",
    "dates = ['2017-01-01', '2017-03-01']\n",
    "sat_list = 'L8'\n",
    "for site in train_sites:\n",
    "   polygon = SDS_tools.polygon_from_kml(os.path.join(filepath_sites,site))\n",
    "   sitename = site[:site.find('.')]  \n",
    "   inputs = {'polygon':polygon, 'dates':dates, 'sat_list':sat_list,\n",
    "             'sitename':sitename, 'filepath':filepath}\n",
    "   print(site)\n",
    "   metadata = SDS_download.retrieve_images(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Label images\n",
    "\n",
    "Label the images into 4 classes: sand, white-water, water and other land features.\n",
    "\n",
    "The labelled images are saved in the *filepath_train* and can be visualised afterwards for quality control. If yo make a mistake, don't worry, this can be fixed later by deleting the labelled image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 46]\n",
      "[46 45]\n",
      "[45 43]\n",
      "[45 41]\n",
      "[103   1]\n",
      "[45 47]\n",
      "[46 46]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "User cancelled labelling images",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d127e333e867>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSDS_download\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inputs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# label images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mSDS_classify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - UNSW\\CoastSat\\coastsat\\SDS_classify.py\u001b[0m in \u001b[0;36mlabel_images\u001b[1;34m(metadata, settings)\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mkey_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pressed'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'escape'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m                     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'User cancelled labelling images'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitforbuttonpress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: User cancelled labelling images"
     ]
    }
   ],
   "source": [
    "# label the images with an interactive annotator\n",
    "%matplotlib qt\n",
    "for site in train_sites:\n",
    "    settings['inputs']['sitename'] = site[:site.find('.')] \n",
    "    # load metadata\n",
    "    metadata = SDS_download.get_metadata(settings['inputs'])\n",
    "    # label images\n",
    "    SDS_classify.label_images(metadata,settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Classifier\n",
    "\n",
    "A Multilayer Perceptron is trained with *scikit-learn*. To train the classifier, the training data needs to be loaded.\n",
    "\n",
    "You can use the data that was labelled here and/or the original CoastSat training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# load the labelled data\n",
    "# initialise the matrix with all the features\n",
    "n_features = 20\n",
    "first_row = np.nan*np.ones((1,n_features))\n",
    "features_matrix = {'sand':first_row, 'white-water':first_row,\n",
    "                   'water':first_row, 'other land features':first_row}\n",
    "# read image labels from each site\n",
    "train_sites = os.listdir(filepath_sites)\n",
    "for site in train_sites:\n",
    "    sitename = site[:site.find('.')] \n",
    "    filepath = os.path.join(filepath_train,sitename)\n",
    "    if os.path.exists(filepath):\n",
    "        list_files = os.listdir(filepath)\n",
    "    else:\n",
    "        continue\n",
    "    # only keep the .pkl files\n",
    "    list_files_pkl = []\n",
    "    for file in list_files:\n",
    "        if '.pkl' in file:\n",
    "            list_files_pkl.append(file)\n",
    "    # load and append the training data to the features matrix\n",
    "    for file in list_files_pkl:\n",
    "        with open(os.path.join(filepath, file), 'rb') as f:\n",
    "            training_data = pickle.load(f)  \n",
    "            for key in training_data['features'].keys():\n",
    "                # check if empty\n",
    "                if len(training_data['features'][key])>0:\n",
    "                    features_matrix[key] = np.append(features_matrix[key],\n",
    "                                training_data['features'][key], axis=0)  \n",
    "# remove the first row (initialized with nans)\n",
    "print('Number of pixels per class in training data:')\n",
    "for key in settings['labels'].keys(): \n",
    "    features_matrix[key] = features_matrix[key][1:,:]\n",
    "    print('%s : %d pixels'%(key,len(features_matrix[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load the original CoastSat dataset (from NSW beaches), which is stored in a .pkl file (you can also combine both):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# you can also load the original CoastSat training data (and optionally merge it with your labelled data)\n",
    "with open(os.path.join(os.getcwd(),'training_data', 'CoastSat_original_training_set_L8.pkl'), 'rb') as f:\n",
    "    features_matrix_original = pickle.load(f)\n",
    "features_matrix = features_matrix_original # comment this line if you want to merge the two datasets and uncomment the line that is commented below\n",
    "print('Number of pixels per class in training data:')\n",
    "for key in features_matrix.keys():\n",
    "#     features_matrix[key] = np.append(features_matrix[key], features_matrix_original[key], axis=0)\n",
    "    print('%s : %d pixels'%(key,len(features_matrix[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the classes do not have the same number of pixels, it is good practice to subsample the very large classes (in this case 'water' and 'other land features'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# subsample randomly the land and water classes\n",
    "n_samples = 7000 # as the most important class is 'sand', this value should be close to the number of sand pixels\n",
    "features_matrix['water'] =  features_matrix['water'][np.random.choice(features_matrix['water'].shape[0],\n",
    "             n_samples, replace=False),:]\n",
    "features_matrix['other land features'] =  features_matrix['other land features'][np.random.choice(features_matrix['other land features'].shape[0],\n",
    "             n_samples, replace=False),:]\n",
    "for key in features_matrix.keys():\n",
    "    print('%s : %d pixels'%(key,len(features_matrix[key])))\n",
    "    \n",
    "# combine into X matrix of features and y vector with the corresponding labels (for each row of X)\n",
    "X = first_row\n",
    "y = np.nan*np.ones((1,1))\n",
    "label_names = ['sand','white-water','water','other land features']\n",
    "labels = [1,2,3,0]\n",
    "for i,key in enumerate(label_names):\n",
    "    y = np.append(y, labels[i]*np.ones((features_matrix[key].shape[0],1)), axis=0)\n",
    "    X = np.append(X, features_matrix[key], axis=0)\n",
    "X = X[1:,:]\n",
    "X[np.isnan(X)] = 1e-9 # nan values will break the training algorithms\n",
    "y = y[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the dataset into train and test: train on 70% of the data and evaluate on the other 30%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# divide in train and test and evaluate the classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)\n",
    "classifier = MLPClassifier(solver='adam')\n",
    "classifier.fit(X_train,y_train)\n",
    "print(classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more robust evaluation is 10-fold cross-validation (may take a few minutes to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "scores = cross_val_score(classifier, X, y, cv=10)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot non-normalized confusion matrix\n",
    "%matplotlib inline\n",
    "y_pred = classifier.predict(X_test)\n",
    "SDS_classify.plot_confusion_matrix(y_test, y_pred,\n",
    "                                   classes=['other land features','sand','white-water','water'],\n",
    "                                   normalize=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When satisfied with the accuracy and confusion matrix, train the model using ALL the training data and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save the final classifier\n",
    "clf_final = MLPClassifier(solver='adam')\n",
    "clf_final.fit(X,y)\n",
    "joblib.dump(clf_final, os.path.join(os.getcwd(), 'classifiers', 'NN_4classes_L8_test.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visually evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifier to be tested\n",
    "%matplotlib qt\n",
    "classifier = joblib.load(os.path.join(os.getcwd(), 'classifiers', 'NN_4classes_L8_test.pkl'))\n",
    "# visualise the classified images\n",
    "%matplotlib qt\n",
    "for site in train_sites:\n",
    "    settings['inputs']['sitename'] = site[:site.find('.')] \n",
    "    # load metadata\n",
    "    metadata = SDS_download.get_metadata(settings['inputs'])\n",
    "    # plot the classified images\n",
    "    SDS_classify.check_classifier(clf_final,metadata,settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
